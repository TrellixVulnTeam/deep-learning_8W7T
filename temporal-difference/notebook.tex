
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Temporal\_Difference}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Mini Project: Temporal-Difference
Methods}\label{mini-project-temporal-difference-methods}

In this notebook, you will write your own implementations of many
Temporal-Difference (TD) methods.

While we have provided some starter code, you are welcome to erase these
hints and write your code from scratch.

    \subsubsection{Part 0: Explore
CliffWalkingEnv}\label{part-0-explore-cliffwalkingenv}

Use the code cell below to create an instance of the
\href{https://github.com/openai/gym/blob/master/gym/envs/toy_text/cliffwalking.py}{CliffWalking}
environment.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{import} \PY{n+nn}{gym}
        \PY{n}{env} \PY{o}{=} \PY{n}{gym}\PY{o}{.}\PY{n}{make}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CliffWalking\PYZhy{}v0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    The agent moves through a \(4\times 12\) gridworld, with states numbered
as follows:

\begin{verbatim}
[[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],
 [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],
 [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],
 [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]]
\end{verbatim}

At the start of any episode, state \texttt{36} is the initial state.
State \texttt{47} is the only terminal state, and the cliff corresponds
to states \texttt{37} through \texttt{46}.

The agent has 4 potential actions:

\begin{verbatim}
UP = 0
RIGHT = 1
DOWN = 2
LEFT = 3
\end{verbatim}

Thus, \(\mathcal{S}^+=\{0, 1, \ldots, 47\}\), and
\(\mathcal{A} =\{0, 1, 2, 3\}\). Verify this by running the code cell
below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{action\PYZus{}space}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{observation\PYZus{}space}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Discrete(4)
Discrete(48)

    \end{Verbatim}

    In this mini-project, we will build towards finding the optimal policy
for the CliffWalking environment. The optimal state-value function is
visualized below. Please take the time now to make sure that you
understand \emph{why} this is the optimal state-value function.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{from} \PY{n+nn}{plot\PYZus{}utils} \PY{k}{import} \PY{n}{plot\PYZus{}values}
        
        \PY{c+c1}{\PYZsh{} define the optimal state\PYZhy{}value function}
        \PY{n}{V\PYZus{}opt} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{12}\PY{p}{)}\PY{p}{)}
        \PY{n}{V\PYZus{}opt}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{13}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{15}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
        \PY{n}{V\PYZus{}opt}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{13}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{15}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{+} \PY{l+m+mi}{1}
        \PY{n}{V\PYZus{}opt}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{13}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{15}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{+} \PY{l+m+mi}{2}
        \PY{n}{V\PYZus{}opt}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{13}
        
        \PY{n}{plot\PYZus{}values}\PY{p}{(}\PY{n}{V\PYZus{}opt}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)

    \end{Verbatim}

    
    \begin{verbatim}
<Figure size 1500x500 with 1 Axes>
    \end{verbatim}

    
    \subsubsection{Part 1: TD Prediction: State
Values}\label{part-1-td-prediction-state-values}

In this section, you will write your own implementation of TD prediction
(for estimating the state-value function).

We will begin by investigating a policy where the agent moves: -
\texttt{RIGHT} in states \texttt{0} through \texttt{10}, inclusive,\\
- \texttt{DOWN} in states \texttt{11}, \texttt{23}, and \texttt{35}, and
- \texttt{UP} in states \texttt{12} through \texttt{22}, inclusive,
states \texttt{24} through \texttt{34}, inclusive, and state
\texttt{36}.

The policy is specified and printed below. Note that states where the
agent does not choose an action have been marked with \texttt{-1}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{policy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{l+m+mi}{11}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{l+m+mi}{11}\PY{p}{)}\PY{p}{]}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = \PYZhy{}1):}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{policy}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{12}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):
[[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.]
 [ 0. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]]

    \end{Verbatim}

    Run the next cell to visualize the state-value function that corresponds
to this policy. Make sure that you take the time to understand why this
is the corresponding value function!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{V\PYZus{}true} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{12}\PY{p}{)}\PY{p}{)}
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{:}
            \PY{n}{V\PYZus{}true}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{12}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{15}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{i}
        \PY{n}{V\PYZus{}true}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{11}\PY{p}{]} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{2}
        \PY{n}{V\PYZus{}true}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{11}\PY{p}{]} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
        \PY{n}{V\PYZus{}true}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{17}
        
        \PY{n}{plot\PYZus{}values}\PY{p}{(}\PY{n}{V\PYZus{}true}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_10_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The above figure is what you will try to approximate through the TD
prediction algorithm.

Your algorithm for TD prediction has five arguments: - \texttt{env}:
This is an instance of an OpenAI Gym environment. -
\texttt{num\_episodes}: This is the number of episodes that are
generated through agent-environment interaction. - \texttt{policy}: This
is a 1D numpy array with \texttt{policy.shape} equal to the number of
states (\texttt{env.nS}). \texttt{policy{[}s{]}} returns the action that
the agent chooses when in state \texttt{s}. - \texttt{alpha}: This is
the step-size parameter for the update step. - \texttt{gamma}: This is
the discount rate. It must be a value between 0 and 1, inclusive
(default value: \texttt{1}).

The algorithm returns as output: - \texttt{V}: This is a dictionary
where \texttt{V{[}s{]}} is the estimated value of state \texttt{s}.

Please complete the function in the code cell below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k+kn}{from} \PY{n+nn}{collections} \PY{k}{import} \PY{n}{defaultdict}\PY{p}{,} \PY{n}{deque}
         \PY{k+kn}{import} \PY{n+nn}{sys}
         
         \PY{k}{def} \PY{n+nf}{td\PYZus{}prediction}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{num\PYZus{}episodes}\PY{p}{,} \PY{n}{policy}\PY{p}{,} \PY{n}{alpha}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} initialize empty dictionaries of floats}
             \PY{n}{V} \PY{o}{=} \PY{n}{defaultdict}\PY{p}{(}\PY{n+nb}{float}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} loop over episodes}
             \PY{k}{for} \PY{n}{i\PYZus{}episode} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{num\PYZus{}episodes}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} monitor progress}
                 \PY{k}{if} \PY{n}{i\PYZus{}episode} \PY{o}{\PYZpc{}} \PY{l+m+mi}{100} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                     \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}r}\PY{l+s+s2}{Episode }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{/}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{i\PYZus{}episode}\PY{p}{,} \PY{n}{num\PYZus{}episodes}\PY{p}{)}\PY{p}{,} \PY{n}{end}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                     \PY{n}{sys}\PY{o}{.}\PY{n}{stdout}\PY{o}{.}\PY{n}{flush}\PY{p}{(}\PY{p}{)}
                 
                 \PY{n}{state} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{p}{)}
                 \PY{k}{while} \PY{k+kc}{True}\PY{p}{:}
                     \PY{n}{action} \PY{o}{=} \PY{n}{policy}\PY{p}{[}\PY{n}{state}\PY{p}{]}
                     \PY{n}{nextState}\PY{p}{,} \PY{n}{r}\PY{p}{,} \PY{n}{done}\PY{p}{,} \PY{n}{info} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{n}{action}\PY{p}{)}
                     \PY{n}{V}\PY{p}{[}\PY{n}{state}\PY{p}{]} \PY{o}{=} \PY{n}{V}\PY{p}{[}\PY{n}{state}\PY{p}{]} \PY{o}{+} \PY{p}{(}\PY{n}{alpha} \PY{o}{*} \PY{p}{(}\PY{n}{r} \PY{o}{+} \PY{p}{(}\PY{n}{gamma} \PY{o}{*} \PY{n}{V}\PY{p}{[}\PY{n}{nextState}\PY{p}{]}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{V}\PY{p}{[}\PY{n}{state}\PY{p}{]}\PY{p}{)}\PY{p}{)}
                     \PY{n}{state} \PY{o}{=} \PY{n}{nextState}
         
                     \PY{k}{if} \PY{n}{done}\PY{p}{:}
                         \PY{k}{break}
         
             \PY{k}{return} \PY{n}{V} 
\end{Verbatim}


    Run the code cell below to test your implementation and visualize the
estimated state-value function. If the code cell returns
\textbf{PASSED}, then you have implemented the function correctly! Feel
free to change the \texttt{num\_episodes} and \texttt{alpha} parameters
that are supplied to the function. However, if you'd like to ensure the
accuracy of the unit test, please do not change the value of
\texttt{gamma} from the default.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k+kn}{import} \PY{n+nn}{check\PYZus{}test}
         
         \PY{c+c1}{\PYZsh{} evaluate the policy and reshape the state\PYZhy{}value function}
         \PY{n}{V\PYZus{}pred} \PY{o}{=} \PY{n}{td\PYZus{}prediction}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{l+m+mi}{5000}\PY{p}{,} \PY{n}{policy}\PY{p}{,} \PY{o}{.}\PY{l+m+mi}{01}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} please do not change the code below this line}
         \PY{n}{V\PYZus{}pred\PYZus{}plot} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{[}\PY{n}{V\PYZus{}pred}\PY{p}{[}\PY{n}{key}\PY{p}{]} \PY{k}{if} \PY{n}{key} \PY{o+ow}{in} \PY{n}{V\PYZus{}pred} \PY{k}{else} \PY{l+m+mi}{0} \PY{k}{for} \PY{n}{key} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{48}\PY{p}{)}\PY{p}{]}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{12}\PY{p}{)}\PY{p}{)} 
         \PY{n}{check\PYZus{}test}\PY{o}{.}\PY{n}{run\PYZus{}check}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{td\PYZus{}prediction\PYZus{}check}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{V\PYZus{}pred\PYZus{}plot}\PY{p}{)}
         \PY{n}{plot\PYZus{}values}\PY{p}{(}\PY{n}{V\PYZus{}pred\PYZus{}plot}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Episode 5000/5000
    \end{Verbatim}

    \textbf{{PASSED}}

    
    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_14_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    How close is your estimated state-value function to the true state-value
function corresponding to the policy?

You might notice that some of the state values are not estimated by the
agent. This is because under this policy, the agent will not visit all
of the states. In the TD prediction algorithm, the agent can only
estimate the values corresponding to states that are visited.

    \subsubsection{Part 2: TD Control: Sarsa}\label{part-2-td-control-sarsa}

In this section, you will write your own implementation of the Sarsa
control algorithm.

Your algorithm has four arguments: - \texttt{env}: This is an instance
of an OpenAI Gym environment. - \texttt{num\_episodes}: This is the
number of episodes that are generated through agent-environment
interaction. - \texttt{alpha}: This is the step-size parameter for the
update step. - \texttt{gamma}: This is the discount rate. It must be a
value between 0 and 1, inclusive (default value: \texttt{1}).

The algorithm returns as output: - \texttt{Q}: This is a dictionary (of
one-dimensional arrays) where \texttt{Q{[}s{]}{[}a{]}} is the estimated
action value corresponding to state \texttt{s} and action \texttt{a}.

Please complete the function in the code cell below.

(\emph{Feel free to define additional functions to help you to organize
your code.})

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}85}]:} \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
         \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}91}]:} \PY{k}{def} \PY{n+nf}{update\PYZus{}Q}\PY{p}{(}\PY{n}{Qsa}\PY{p}{,} \PY{n}{Qsa\PYZus{}next}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{alpha}\PY{p}{,} \PY{n}{gamma}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} updates the action\PYZhy{}value function estimate using the most recent time step \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{return} \PY{n}{Qsa} \PY{o}{+} \PY{p}{(}\PY{n}{alpha} \PY{o}{*} \PY{p}{(}\PY{n}{reward} \PY{o}{+} \PY{p}{(}\PY{n}{gamma} \PY{o}{*} \PY{n}{Qsa\PYZus{}next}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{Qsa}\PY{p}{)}\PY{p}{)}
         
         \PY{k}{def} \PY{n+nf}{epsilon\PYZus{}greedy\PYZus{}probs}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{Q\PYZus{}s}\PY{p}{,} \PY{n}{i\PYZus{}episode}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} obtains the action probabilities corresponding to epsilon\PYZhy{}greedy policy \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{epsilon} \PY{o}{=} \PY{l+m+mf}{1.0} \PY{o}{/} \PY{n}{i\PYZus{}episode}
             \PY{k}{if} \PY{n}{eps} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
                 \PY{n}{epsilon} \PY{o}{=} \PY{n}{eps}
             \PY{n}{policy\PYZus{}s} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{nA}\PY{p}{)} \PY{o}{*} \PY{n}{epsilon} \PY{o}{/} \PY{n}{env}\PY{o}{.}\PY{n}{nA}
             \PY{n}{policy\PYZus{}s}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{Q\PYZus{}s}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{epsilon} \PY{o}{+} \PY{p}{(}\PY{n}{epsilon} \PY{o}{/} \PY{n}{env}\PY{o}{.}\PY{n}{nA}\PY{p}{)}
             \PY{k}{return} \PY{n}{policy\PYZus{}s}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{k}{def} \PY{n+nf}{updateQ}\PY{p}{(}\PY{n}{Qsa}\PY{p}{,} \PY{n}{Qsa\PYZus{}next}\PY{p}{,} \PY{n}{r}\PY{p}{,} \PY{n}{alpha}\PY{p}{,} \PY{n}{gamma}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} updates the action\PYZhy{}value function estimate using the most recent time step \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{return} \PY{n}{Qsa} \PY{o}{+} \PY{p}{(}\PY{n}{alpha} \PY{o}{*} \PY{p}{(}\PY{n}{r} \PY{o}{+}\PY{p}{(}\PY{n}{gamma} \PY{o}{*} \PY{n}{Qsa\PYZus{}next}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{Qsa}\PY{p}{)}\PY{p}{)}
         
         \PY{k}{def} \PY{n+nf}{greedy\PYZus{}probs}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{Qs}\PY{p}{,} \PY{n}{episode\PYZus{}i}\PY{p}{,} \PY{n}{eps} \PY{o}{=} \PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} obtains the action probabilities corresponding to epsilon\PYZhy{}greedy policy \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{epsilon} \PY{o}{=} \PY{l+m+mf}{1.0} \PY{o}{/} \PY{n}{episode\PYZus{}i}
             \PY{k}{if} \PY{n}{eps} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
                 \PY{n}{epsilion} \PY{o}{=} \PY{n}{eps}
                 
             \PY{n}{policy\PYZus{}s} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{nA}\PY{p}{)} \PY{o}{*} \PY{n}{epsilon} \PY{o}{/} \PY{n}{env}\PY{o}{.}\PY{n}{nA}
             \PY{n}{policy\PYZus{}s}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{Qs}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{epsilon} \PY{o}{+} \PY{p}{(}\PY{n}{epsilon}\PY{o}{/}\PY{n}{env}\PY{o}{.}\PY{n}{nA}\PY{p}{)}
             \PY{k}{return} \PY{n}{policy\PYZus{}s}
         
         
         \PY{k}{def} \PY{n+nf}{sarsa}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{num\PYZus{}episodes}\PY{p}{,} \PY{n}{alpha}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} initialize action\PYZhy{}value function (empty dictionary of arrays)}
             \PY{n}{Q} \PY{o}{=} \PY{n}{defaultdict}\PY{p}{(}\PY{k}{lambda}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{nA}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} initialize performance monitor}
             \PY{n}{plot\PYZus{}every} \PY{o}{=} \PY{l+m+mi}{100}
             \PY{n}{tmp\PYZus{}scores} \PY{o}{=} \PY{n}{deque}\PY{p}{(}\PY{n}{maxlen}\PY{o}{=}\PY{n}{plot\PYZus{}every}\PY{p}{)}
             \PY{n}{scores} \PY{o}{=} \PY{n}{deque}\PY{p}{(}\PY{n}{maxlen}\PY{o}{=}\PY{n}{num\PYZus{}episodes}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} loop over episodes}
             \PY{k}{for} \PY{n}{i\PYZus{}episode} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{num\PYZus{}episodes}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} monitor progress}
                 \PY{k}{if} \PY{n}{i\PYZus{}episode} \PY{o}{\PYZpc{}} \PY{l+m+mi}{100} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                     \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}r}\PY{l+s+s2}{Episode }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{/}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{i\PYZus{}episode}\PY{p}{,} \PY{n}{num\PYZus{}episodes}\PY{p}{)}\PY{p}{,} \PY{n}{end}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                     \PY{n}{sys}\PY{o}{.}\PY{n}{stdout}\PY{o}{.}\PY{n}{flush}\PY{p}{(}\PY{p}{)}   
                 
                 \PY{n}{state} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{p}{)}
                 \PY{n}{score} \PY{o}{=} \PY{l+m+mi}{0}
                 \PY{n}{policy\PYZus{}s} \PY{o}{=} \PY{n}{greedy\PYZus{}probs}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{Q}\PY{p}{[}\PY{n}{state}\PY{p}{]}\PY{p}{,} \PY{n}{i\PYZus{}episode}\PY{p}{)}
                 \PY{n}{action} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{nA}\PY{p}{)}\PY{p}{,} \PY{n}{p}\PY{o}{=}\PY{n}{policy\PYZus{}s}\PY{p}{)}
                 
                 
                 \PY{k}{for} \PY{n}{step} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{465}\PY{p}{)}\PY{p}{:}
                     \PY{n}{next\PYZus{}state}\PY{p}{,} \PY{n}{r}\PY{p}{,} \PY{n}{done}\PY{p}{,} \PY{n}{info} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{n}{action}\PY{p}{)}
                     \PY{n}{score} \PY{o}{+}\PY{o}{=} \PY{n}{r}
                     
                     \PY{k}{if} \PY{o+ow}{not} \PY{n}{done}\PY{p}{:}
                         \PY{n}{policy\PYZus{}s} \PY{o}{=} \PY{n}{greedy\PYZus{}probs}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{Q}\PY{p}{[}\PY{n}{next\PYZus{}state}\PY{p}{]}\PY{p}{,} \PY{n}{i\PYZus{}episode}\PY{p}{)}
                         \PY{n}{next\PYZus{}action} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{nA}\PY{p}{)}\PY{p}{,} \PY{n}{p}\PY{o}{=}\PY{n}{policy\PYZus{}s}\PY{p}{)}
                         \PY{n}{Q}\PY{p}{[}\PY{n}{state}\PY{p}{]}\PY{p}{[}\PY{n}{action}\PY{p}{]} \PY{o}{=} \PY{n}{updateQ}\PY{p}{(}\PY{n}{Q}\PY{p}{[}\PY{n}{state}\PY{p}{]}\PY{p}{[}\PY{n}{action}\PY{p}{]}\PY{p}{,} \PY{n}{Q}\PY{p}{[}\PY{n}{next\PYZus{}state}\PY{p}{]}\PY{p}{[}\PY{n}{next\PYZus{}action}\PY{p}{]}\PY{p}{,}
                                                   \PY{n}{r}\PY{p}{,} \PY{n}{alpha}\PY{p}{,} \PY{n}{gamma}\PY{p}{)}
                         \PY{n}{state} \PY{o}{=} \PY{n}{next\PYZus{}state}
                         \PY{n}{action} \PY{o}{=} \PY{n}{next\PYZus{}action}
                     \PY{k}{if} \PY{n}{done}\PY{p}{:}
                         \PY{n}{Q}\PY{p}{[}\PY{n}{state}\PY{p}{]}\PY{p}{[}\PY{n}{action}\PY{p}{]} \PY{o}{=} \PY{n}{updateQ}\PY{p}{(}\PY{n}{Q}\PY{p}{[}\PY{n}{state}\PY{p}{]}\PY{p}{[}\PY{n}{action}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{r}\PY{p}{,} \PY{n}{alpha}\PY{p}{,} \PY{n}{gamma}\PY{p}{)}
                         \PY{n}{tmp\PYZus{}scores}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{score}\PY{p}{)}
                         \PY{k}{break}
                         
                     \PY{k}{if} \PY{n}{i\PYZus{}episode} \PY{o}{\PYZpc{}} \PY{n}{plot\PYZus{}every} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                         \PY{n}{scores}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{tmp\PYZus{}scores}\PY{p}{)}\PY{p}{)}
             
             
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{num\PYZus{}episodes}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{scores}\PY{p}{)}\PY{p}{,}\PY{n}{endpoint}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{scores}\PY{p}{)}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Episode Number}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Avg Reward (Over next }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ Episodes)}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{plot\PYZus{}every}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
             
             \PY{n+nb}{print}\PY{p}{(}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Best Avg Reward over }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ Episodes: }\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}}\PY{k}{plot\PYZus{}every}), np.max(scores))
             
             \PY{k}{return} \PY{n}{Q}
\end{Verbatim}


    Use the next code cell to visualize the \textbf{\emph{estimated}}
optimal policy and the corresponding state-value function.

If the code cell returns \textbf{PASSED}, then you have implemented the
function correctly! Feel free to change the \texttt{num\_episodes} and
\texttt{alpha} parameters that are supplied to the function. However, if
you'd like to ensure the accuracy of the unit test, please do not change
the value of \texttt{gamma} from the default.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{c+c1}{\PYZsh{} obtain the estimated optimal policy and corresponding action\PYZhy{}value function}
         \PY{n}{Q\PYZus{}sarsa} \PY{o}{=} \PY{n}{sarsa}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{l+m+mi}{5000}\PY{p}{,} \PY{o}{.}\PY{l+m+mi}{01}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} print the estimated optimal policy}
         \PY{n}{policy\PYZus{}sarsa} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{Q\PYZus{}sarsa}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{p}{)} \PY{k}{if} \PY{n}{key} \PY{o+ow}{in} \PY{n}{Q\PYZus{}sarsa} \PY{k}{else} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1} \PY{k}{for} \PY{n}{key} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{48}\PY{p}{)}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{12}\PY{p}{)}
         \PY{n}{check\PYZus{}test}\PY{o}{.}\PY{n}{run\PYZus{}check}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{td\PYZus{}control\PYZus{}check}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{policy\PYZus{}sarsa}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Estimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = \PYZhy{}1):}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{policy\PYZus{}sarsa}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} plot the estimated optimal state\PYZhy{}value function}
         \PY{n}{V\PYZus{}sarsa} \PY{o}{=} \PY{p}{(}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{Q\PYZus{}sarsa}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{p}{)} \PY{k}{if} \PY{n}{key} \PY{o+ow}{in} \PY{n}{Q\PYZus{}sarsa} \PY{k}{else} \PY{l+m+mi}{0} \PY{k}{for} \PY{n}{key} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{48}\PY{p}{)}\PY{p}{]}\PY{p}{)}
         \PY{n}{plot\PYZus{}values}\PY{p}{(}\PY{n}{V\PYZus{}sarsa}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Episode 5000/5000
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_21_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Best Avg Reward over 100 Episodes:  -13.0

    \end{Verbatim}

    \textbf{{PASSED}}

    
    \begin{Verbatim}[commandchars=\\\{\}]

Estimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):
[[ 1  3  0  3  1  0  1  1  1  2  1  1]
 [ 0  0  1  1  1  1  1  0  1  1  1  2]
 [ 1  1  1  1  1  1  1  1  1  1  1  2]
 [ 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_21_6.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Part 3: TD Control:
Q-learning}\label{part-3-td-control-q-learning}

In this section, you will write your own implementation of the
Q-learning control algorithm.

Your algorithm has four arguments: - \texttt{env}: This is an instance
of an OpenAI Gym environment. - \texttt{num\_episodes}: This is the
number of episodes that are generated through agent-environment
interaction. - \texttt{alpha}: This is the step-size parameter for the
update step. - \texttt{gamma}: This is the discount rate. It must be a
value between 0 and 1, inclusive (default value: \texttt{1}).

The algorithm returns as output: - \texttt{Q}: This is a dictionary (of
one-dimensional arrays) where \texttt{Q{[}s{]}{[}a{]}} is the estimated
action value corresponding to state \texttt{s} and action \texttt{a}.

Please complete the function in the code cell below.

(\emph{Feel free to define additional functions to help you to organize
your code.})

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{k}{def} \PY{n+nf}{q\PYZus{}learning}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{num\PYZus{}episodes}\PY{p}{,} \PY{n}{alpha}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} initialize empty dictionary of arrays}
             \PY{n}{Q} \PY{o}{=} \PY{n}{defaultdict}\PY{p}{(}\PY{k}{lambda}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{nA}\PY{p}{)}\PY{p}{)}
             \PY{n}{plot\PYZus{}every} \PY{o}{=} \PY{l+m+mi}{100}
             \PY{n}{aux\PYZus{}scores} \PY{o}{=} \PY{n}{deque}\PY{p}{(}\PY{n}{maxlen}\PY{o}{=}\PY{n}{plot\PYZus{}every}\PY{p}{)}
             \PY{n}{scores} \PY{o}{=} \PY{n}{deque}\PY{p}{(}\PY{n}{maxlen}\PY{o}{=}\PY{n}{num\PYZus{}episodes}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} loop over episodes}
             \PY{k}{for} \PY{n}{i\PYZus{}episode} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{num\PYZus{}episodes}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} monitor progress}
                 \PY{k}{if} \PY{n}{i\PYZus{}episode} \PY{o}{\PYZpc{}} \PY{l+m+mi}{100} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                     \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}r}\PY{l+s+s2}{Episode }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{/}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{i\PYZus{}episode}\PY{p}{,} \PY{n}{num\PYZus{}episodes}\PY{p}{)}\PY{p}{,} \PY{n}{end}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                     \PY{n}{sys}\PY{o}{.}\PY{n}{stdout}\PY{o}{.}\PY{n}{flush}\PY{p}{(}\PY{p}{)}
                 \PY{n}{score} \PY{o}{=} \PY{l+m+mi}{0}
                 \PY{n}{state} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{p}{)}
                 
                 \PY{k}{while} \PY{k+kc}{True}\PY{p}{:}
                     \PY{n}{policy\PYZus{}s} \PY{o}{=} \PY{n}{greedy\PYZus{}probs}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{Q}\PY{p}{[}\PY{n}{state}\PY{p}{]}\PY{p}{,} \PY{n}{i\PYZus{}episode}\PY{p}{)}
                     \PY{n}{action} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{nA}\PY{p}{)}\PY{p}{,} \PY{n}{p}\PY{o}{=}\PY{n}{policy\PYZus{}s}\PY{p}{)}
                     \PY{n}{next\PYZus{}state}\PY{p}{,} \PY{n}{r}\PY{p}{,} \PY{n}{done}\PY{p}{,} \PY{n}{info} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{n}{action}\PY{p}{)}
                     \PY{n}{score} \PY{o}{+}\PY{o}{=} \PY{n}{r}
                     
                     \PY{n}{Q}\PY{p}{[}\PY{n}{state}\PY{p}{]}\PY{p}{[}\PY{n}{action}\PY{p}{]} \PY{o}{=} \PY{n}{updateQ}\PY{p}{(}\PY{n}{Q}\PY{p}{[}\PY{n}{state}\PY{p}{]}\PY{p}{[}\PY{n}{action}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{Q}\PY{p}{[}\PY{n}{next\PYZus{}state}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{r}\PY{p}{,} \PY{n}{alpha}\PY{p}{,} \PY{n}{gamma}\PY{p}{)}
                     
                     \PY{n}{state} \PY{o}{=} \PY{n}{next\PYZus{}state}
                     
                     \PY{k}{if} \PY{n}{done}\PY{p}{:} 
                         \PY{n}{aux\PYZus{}scores}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{score}\PY{p}{)}
                         \PY{k}{break}
                 \PY{k}{if} \PY{n}{i\PYZus{}episode} \PY{o}{\PYZpc{}} \PY{n}{plot\PYZus{}every} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                     \PY{n}{scores}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{aux\PYZus{}scores}\PY{p}{)}\PY{p}{)}
                     
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{num\PYZus{}episodes}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{scores}\PY{p}{)}\PY{p}{,} \PY{n}{endpoint}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{scores}\PY{p}{)}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Episode Number}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Average Reward (Over Next }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ Episodes)}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{plot\PYZus{}every}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
             
             \PY{n+nb}{print}\PY{p}{(}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Best Average Reward over }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ Episodes: }\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{plot\PYZus{}every}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{scores}\PY{p}{)}\PY{p}{)}
                 
             \PY{k}{return} \PY{n}{Q}
\end{Verbatim}


    Use the next code cell to visualize the \textbf{\emph{estimated}}
optimal policy and the corresponding state-value function.

If the code cell returns \textbf{PASSED}, then you have implemented the
function correctly! Feel free to change the \texttt{num\_episodes} and
\texttt{alpha} parameters that are supplied to the function. However, if
you'd like to ensure the accuracy of the unit test, please do not change
the value of \texttt{gamma} from the default.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{c+c1}{\PYZsh{} obtain the estimated optimal policy and corresponding action\PYZhy{}value function}
         \PY{n}{Q\PYZus{}sarsamax} \PY{o}{=} \PY{n}{q\PYZus{}learning}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{l+m+mi}{5000}\PY{p}{,} \PY{o}{.}\PY{l+m+mi}{01}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} print the estimated optimal policy}
         \PY{n}{policy\PYZus{}sarsamax} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{Q\PYZus{}sarsamax}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{p}{)} \PY{k}{if} \PY{n}{key} \PY{o+ow}{in} \PY{n}{Q\PYZus{}sarsamax} \PY{k}{else} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1} \PY{k}{for} \PY{n}{key} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{48}\PY{p}{)}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{12}\PY{p}{)}\PY{p}{)}
         \PY{n}{check\PYZus{}test}\PY{o}{.}\PY{n}{run\PYZus{}check}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{td\PYZus{}control\PYZus{}check}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{policy\PYZus{}sarsamax}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Estimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = \PYZhy{}1):}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{policy\PYZus{}sarsamax}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} plot the estimated optimal state\PYZhy{}value function}
         \PY{n}{plot\PYZus{}values}\PY{p}{(}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{Q\PYZus{}sarsamax}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{p}{)} \PY{k}{if} \PY{n}{key} \PY{o+ow}{in} \PY{n}{Q\PYZus{}sarsamax} \PY{k}{else} \PY{l+m+mi}{0} \PY{k}{for} \PY{n}{key} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{48}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Episode 5000/5000
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_25_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Best Average Reward over 100 Episodes:  -13.0

    \end{Verbatim}

    \textbf{{PASSED}}

    
    \begin{Verbatim}[commandchars=\\\{\}]

Estimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):
[[ 3  1  1  2  0  1  1  3  2  2  1  0]
 [ 3  1  0  3  1  2  1  2  2  2  1  2]
 [ 1  1  1  1  1  1  1  1  1  1  1  2]
 [ 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0]]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_25_6.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Part 4: TD Control: Expected
Sarsa}\label{part-4-td-control-expected-sarsa}

In this section, you will write your own implementation of the Expected
Sarsa control algorithm.

Your algorithm has four arguments: - \texttt{env}: This is an instance
of an OpenAI Gym environment. - \texttt{num\_episodes}: This is the
number of episodes that are generated through agent-environment
interaction. - \texttt{alpha}: This is the step-size parameter for the
update step. - \texttt{gamma}: This is the discount rate. It must be a
value between 0 and 1, inclusive (default value: \texttt{1}).

The algorithm returns as output: - \texttt{Q}: This is a dictionary (of
one-dimensional arrays) where \texttt{Q{[}s{]}{[}a{]}} is the estimated
action value corresponding to state \texttt{s} and action \texttt{a}.

Please complete the function in the code cell below.

(\emph{Feel free to define additional functions to help you to organize
your code.})

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}92}]:} \PY{k}{def} \PY{n+nf}{expected\PYZus{}sarsa}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{num\PYZus{}episodes}\PY{p}{,} \PY{n}{alpha}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} initialize empty dictionary of arrays}
             \PY{n}{Q} \PY{o}{=} \PY{n}{defaultdict}\PY{p}{(}\PY{k}{lambda}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{nA}\PY{p}{)}\PY{p}{)}
             \PY{n}{plot\PYZus{}every} \PY{o}{=} \PY{l+m+mi}{100}
             \PY{n}{aux\PYZus{}scores} \PY{o}{=} \PY{n}{deque}\PY{p}{(}\PY{n}{maxlen}\PY{o}{=}\PY{n}{plot\PYZus{}every}\PY{p}{)}
             \PY{n}{scores} \PY{o}{=} \PY{n}{deque}\PY{p}{(}\PY{n}{maxlen}\PY{o}{=}\PY{n}{num\PYZus{}episodes}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} loop over episodes}
             \PY{k}{for} \PY{n}{i\PYZus{}episode} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{num\PYZus{}episodes}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} monitor progress}
                 \PY{k}{if} \PY{n}{i\PYZus{}episode} \PY{o}{\PYZpc{}} \PY{l+m+mi}{100} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                     \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}r}\PY{l+s+s2}{Episode }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{/}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{i\PYZus{}episode}\PY{p}{,} \PY{n}{num\PYZus{}episodes}\PY{p}{)}\PY{p}{,} \PY{n}{end}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                     \PY{n}{sys}\PY{o}{.}\PY{n}{stdout}\PY{o}{.}\PY{n}{flush}\PY{p}{(}\PY{p}{)}
                 
                 \PY{n}{state} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{p}{)}
                 \PY{n}{score} \PY{o}{=} \PY{l+m+mi}{0}
                 \PY{n}{policy\PYZus{}s} \PY{o}{=} \PY{n}{epsilon\PYZus{}greedy\PYZus{}probs}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{Q}\PY{p}{[}\PY{n}{state}\PY{p}{]}\PY{p}{,} \PY{n}{i\PYZus{}episode}\PY{p}{,} \PY{l+m+mf}{0.0022}\PY{p}{)}
                 
                 \PY{k}{while} \PY{k+kc}{True}\PY{p}{:}
                     
                     \PY{n}{action} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{nA}\PY{p}{)}\PY{p}{,} \PY{n}{p}\PY{o}{=}\PY{n}{policy\PYZus{}s}\PY{p}{)}
                     \PY{n}{next\PYZus{}state}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{done}\PY{p}{,} \PY{n}{info} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{n}{action}\PY{p}{)}
                     \PY{n}{score} \PY{o}{+}\PY{o}{=} \PY{n}{reward}
                     \PY{n}{policy\PYZus{}s} \PY{o}{=} \PY{n}{epsilon\PYZus{}greedy\PYZus{}probs}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{Q}\PY{p}{[}\PY{n}{next\PYZus{}state}\PY{p}{]}\PY{p}{,} \PY{n}{i\PYZus{}episode}\PY{p}{,} \PY{l+m+mf}{0.0022}\PY{p}{)}
         
                     \PY{n}{Q}\PY{p}{[}\PY{n}{state}\PY{p}{]}\PY{p}{[}\PY{n}{action}\PY{p}{]} \PY{o}{=} \PY{n}{update\PYZus{}Q}\PY{p}{(}\PY{n}{Q}\PY{p}{[}\PY{n}{state}\PY{p}{]}\PY{p}{[}\PY{n}{action}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{Q}\PY{p}{[}\PY{n}{next\PYZus{}state}\PY{p}{]}\PY{p}{,} \PY{n}{policy\PYZus{}s}\PY{p}{)}\PY{p}{,} \PYZbs{}
                                                           \PY{n}{reward}\PY{p}{,} \PY{n}{alpha}\PY{p}{,} \PY{n}{gamma}\PY{p}{)}       
                     \PY{n}{state} \PY{o}{=} \PY{n}{next\PYZus{}state}
                     \PY{k}{if} \PY{n}{done}\PY{p}{:}
                         \PY{n}{aux\PYZus{}scores}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{score}\PY{p}{)}
                         \PY{k}{break}
                 \PY{k}{if} \PY{p}{(}\PY{n}{i\PYZus{}episode} \PY{o}{\PYZpc{}} \PY{n}{plot\PYZus{}every} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
                     \PY{n}{scores}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{aux\PYZus{}scores}\PY{p}{)}\PY{p}{)}
             
             
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{num\PYZus{}episodes}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{scores}\PY{p}{)}\PY{p}{,}\PY{n}{endpoint}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n}{scores}\PY{p}{)}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Episode Number}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Average Reward (Over Next }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ Episodes)}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{plot\PYZus{}every}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
             
             \PY{n+nb}{print}\PY{p}{(}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Best Average Reward over }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ Episodes: }\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{plot\PYZus{}every}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{scores}\PY{p}{)}\PY{p}{)}
            
             \PY{k}{return} \PY{n}{Q}
\end{Verbatim}


    Use the next code cell to visualize the \textbf{\emph{estimated}}
optimal policy and the corresponding state-value function.

If the code cell returns \textbf{PASSED}, then you have implemented the
function correctly! Feel free to change the \texttt{num\_episodes} and
\texttt{alpha} parameters that are supplied to the function. However, if
you'd like to ensure the accuracy of the unit test, please do not change
the value of \texttt{gamma} from the default.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}93}]:} \PY{c+c1}{\PYZsh{} obtain the estimated optimal policy and corresponding action\PYZhy{}value function}
         \PY{n}{Q\PYZus{}expsarsa} \PY{o}{=} \PY{n}{expected\PYZus{}sarsa}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{l+m+mi}{10000}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} print the estimated optimal policy}
         \PY{n}{policy\PYZus{}expsarsa} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{Q\PYZus{}expsarsa}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{p}{)} \PY{k}{if} \PY{n}{key} \PY{o+ow}{in} \PY{n}{Q\PYZus{}expsarsa} \PY{k}{else} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1} \PY{k}{for} \PY{n}{key} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{48}\PY{p}{)}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{12}\PY{p}{)}
         \PY{n}{check\PYZus{}test}\PY{o}{.}\PY{n}{run\PYZus{}check}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{td\PYZus{}control\PYZus{}check}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{policy\PYZus{}expsarsa}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Estimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = \PYZhy{}1):}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{policy\PYZus{}expsarsa}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} plot the estimated optimal state\PYZhy{}value function}
         \PY{n}{plot\PYZus{}values}\PY{p}{(}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{Q\PYZus{}expsarsa}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{p}{)} \PY{k}{if} \PY{n}{key} \PY{o+ow}{in} \PY{n}{Q\PYZus{}expsarsa} \PY{k}{else} \PY{l+m+mi}{0} \PY{k}{for} \PY{n}{key} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{48}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Episode 10000/10000
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_29_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Best Average Reward over 100 Episodes:  -13.0

    \end{Verbatim}

    \textbf{{PASSED}}

    
    \begin{Verbatim}[commandchars=\\\{\}]

Estimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):
[[ 2  1  1  1  1  1  1  1  2  1  1  2]
 [ 1  1  1  1  1  1  1  1  1  1  1  2]
 [ 1  1  1  1  1  1  1  1  1  1  1  2]
 [ 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0]]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)
C:\textbackslash{}Users\textbackslash{}zee8ca\textbackslash{}AppData\textbackslash{}Local\textbackslash{}Continuum\textbackslash{}anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}matplotlib\textbackslash{}cbook\textbackslash{}deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.
  warnings.warn(message, mplDeprecation, stacklevel=1)

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_29_6.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
